{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Comparison\n",
    "\n",
    "This notebook covers:\n",
    "- Training multiple ML models\n",
    "- Model comparison and evaluation\n",
    "- Hyperparameter tuning\n",
    "- Model selection and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from train_model import ModelTrainer, OutcomePredictionTrainer, TreatmentRecommendationTrainer\n",
    "from evaluate_model import ModelEvaluator, OutcomeEvaluator, TreatmentEvaluator\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data\n",
    "X_train = pd.read_csv('../data/processed/X_train_final.csv')\n",
    "X_test = pd.read_csv('../data/processed/X_test_final.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv')['y_train']\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv')['y_test']\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "# Load feature names\n",
    "selected_features = pd.read_csv('../data/processed/selected_features.csv')['selected_features'].tolist()\n",
    "print(f\"\\nSelected features ({len(selected_features)}): {selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outcome Prediction Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING OUTCOME PREDICTION MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize outcome prediction trainer\n",
    "outcome_trainer = OutcomePredictionTrainer()\n",
    "\n",
    "# Train all models\n",
    "outcome_results, outcome_models = outcome_trainer.train_all_models(X_train, y_train)\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(\"-\" * 40)\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': list(outcome_results.keys()),\n",
    "    'CV_Mean': [results['cv_mean'] for results in outcome_results.values()],\n",
    "    'CV_Std': [results['cv_std'] for results in outcome_results.values()]\n",
    "}).sort_values('CV_Mean', ascending=False)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nBest Model: {outcome_trainer.best_model_name}\")\n",
    "print(f\"Best CV Score: {outcome_trainer.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# CV scores comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "models = list(outcome_results.keys())\n",
    "cv_means = [outcome_results[model]['cv_mean'] for model in models]\n",
    "cv_stds = [outcome_results[model]['cv_std'] for model in models]\n",
    "\n",
    "plt.bar(models, cv_means, yerr=cv_stds, capsize=5)\n",
    "plt.title('Cross-Validation Scores - Outcome Prediction')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot of CV scores\n",
    "plt.subplot(2, 2, 2)\n",
    "cv_scores_data = [outcome_results[model]['cv_scores'] for model in models]\n",
    "plt.boxplot(cv_scores_data, labels=models)\n",
    "plt.title('Cross-Validation Score Distribution')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Model performance on test set\n",
    "plt.subplot(2, 2, 3)\n",
    "test_scores = []\n",
    "for model_name, model in outcome_models.items():\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "plt.bar(models, test_scores)\n",
    "plt.title('Test Set Performance - Outcome Prediction')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# CV vs Test performance\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(cv_means, test_scores, s=100, alpha=0.7)\n",
    "for i, model in enumerate(models):\n",
    "    plt.annotate(model, (cv_means[i], test_scores[i]), xytext=(5, 5), \n",
    "                textcoords='offset points', fontsize=8)\n",
    "plt.plot([min(cv_means), max(cv_means)], [min(cv_means), max(cv_means)], 'r--', alpha=0.5)\n",
    "plt.xlabel('CV Score')\n",
    "plt.ylabel('Test Score')\n",
    "plt.title('CV vs Test Performance')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print test scores\n",
    "print(\"\\nTest Set Performance:\")\n",
    "for model_name, score in zip(models, test_scores):\n",
    "    print(f\"{model_name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tune the best performing model\n",
    "best_model_name = outcome_trainer.best_model_name\n",
    "print(f\"Tuning hyperparameters for: {best_model_name}\")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuned_model = outcome_trainer.hyperparameter_tuning(X_train, y_train, best_model_name)\n",
    "\n",
    "if tuned_model:\n",
    "    # Evaluate tuned model\n",
    "    tuned_cv_scores = outcome_trainer.train_single_model(tuned_model, X_train, y_train, f\"Tuned {best_model_name}\")[1]\n",
    "    tuned_test_score = tuned_model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"\\nOriginal {best_model_name} CV Score: {outcome_trainer.best_score:.4f}\")\n",
    "    print(f\"Tuned {best_model_name} CV Score: {tuned_cv_scores.mean():.4f}\")\n",
    "    print(f\"Improvement: {tuned_cv_scores.mean() - outcome_trainer.best_score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nOriginal {best_model_name} Test Score: {outcome_models[best_model_name].score(X_test, y_test):.4f}\")\n",
    "    print(f\"Tuned {best_model_name} Test Score: {tuned_test_score:.4f}\")\n",
    "    \n",
    "    # Update best model if tuned version is better\n",
    "    if tuned_cv_scores.mean() > outcome_trainer.best_score:\n",
    "        outcome_trainer.best_model = tuned_model\n",
    "        outcome_trainer.best_score = tuned_cv_scores.mean()\n",
    "        print(f\"\\nUpdated best model to tuned version!\")\n",
    "else:\n",
    "    print(f\"Hyperparameter tuning not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = OutcomeEvaluator()\n",
    "\n",
    "# Define class labels\n",
    "outcome_labels = ['Improved', 'Not Improved', 'Stable']  # Adjust based on your encoded labels\n",
    "\n",
    "# Evaluate best model\n",
    "best_model = outcome_trainer.best_model\n",
    "results, cm, feature_imp = evaluator.generate_evaluation_report(\n",
    "    best_model, X_test, y_test, selected_features, outcome_labels, \n",
    "    f\"Best Model: {outcome_trainer.best_model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves for best model\n",
    "print(\"\\nGenerating learning curves...\")\n",
    "evaluator.plot_learning_curves(best_model, X_train, y_train, outcome_trainer.best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models on test set\n",
    "print(\"\\nComparing all models on test set...\")\n",
    "\n",
    "all_results = {}\n",
    "for model_name, model in outcome_models.items():\n",
    "    model_results = evaluator.evaluate_classification_model(model, X_test, y_test, model_name)\n",
    "    all_results[model_name] = model_results\n",
    "\n",
    "# Plot comparison\n",
    "comparison_df = evaluator.plot_model_comparison(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treatment Recommendation Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For treatment recommendation, we need to load the full engineered dataset\n",
    "# and prepare it specifically for treatment recommendation\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPARING DATA FOR TREATMENT RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load full engineered dataset\n",
    "df_engineered = pd.read_csv('../data/processed/engineered_data.csv')\n",
    "\n",
    "# Initialize treatment recommendation trainer\n",
    "treatment_trainer = TreatmentRecommendationTrainer()\n",
    "\n",
    "# Prepare treatment recommendation data (exclude outcome from features)\n",
    "X_treatment, y_treatment = treatment_trainer.prepare_treatment_data(df_engineered)\n",
    "\n",
    "print(f\"Treatment recommendation features: {X_treatment.shape[1]}\")\n",
    "print(f\"Treatment target distribution:\")\n",
    "print(y_treatment.value_counts().sort_index())\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "    X_treatment, y_treatment, test_size=0.2, random_state=42, stratify=y_treatment\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_t = MinMaxScaler()\n",
    "X_train_t_scaled = scaler_t.fit_transform(X_train_t)\n",
    "X_test_t_scaled = scaler_t.transform(X_test_t)\n",
    "\n",
    "print(f\"Treatment training set: {X_train_t_scaled.shape}\")\n",
    "print(f\"Treatment test set: {X_test_t_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING TREATMENT RECOMMENDATION MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train treatment recommendation models\n",
    "treatment_results, treatment_models = treatment_trainer.train_all_models(X_train_t_scaled, y_train_t)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nTreatment Recommendation Model Performance:\")\n",
    "print(\"-\" * 50)\n",
    "treatment_results_df = pd.DataFrame({\n",
    "    'Model': list(treatment_results.keys()),\n",
    "    'CV_Mean': [results['cv_mean'] for results in treatment_results.values()],\n",
    "    'CV_Std': [results['cv_std'] for results in treatment_results.values()]\n",
    "}).sort_values('CV_Mean', ascending=False)\n",
    "\n",
    "print(treatment_results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nBest Treatment Model: {treatment_trainer.best_model_name}\")\n",
    "print(f\"Best CV Score: {treatment_trainer.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate treatment recommendation model\n",
    "treatment_evaluator = TreatmentEvaluator()\n",
    "treatment_labels = ['Treatment A', 'Treatment B', 'Treatment C', 'Treatment D']  # Adjust based on your data\n",
    "\n",
    "treatment_eval_results = treatment_evaluator.evaluate_treatment_recommendations(\n",
    "    treatment_trainer.best_model, X_test_t_scaled, y_test_t, \n",
    "    treatment_labels, f\"Best Treatment Model: {treatment_trainer.best_model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING TRAINED MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save outcome prediction model\n",
    "outcome_trainer.save_model(outcome_trainer.best_model, '../models/outcome_prediction_model.pkl')\n",
    "\n",
    "# Save treatment recommendation model\n",
    "treatment_trainer.save_model(treatment_trainer.best_model, '../models/treatment_recommendation_model.pkl')\n",
    "\n",
    "# Save scalers\n",
    "import pickle\n",
    "with open('../models/outcome_scaler.pkl', 'wb') as f:\n",
    "    # Note: In a real implementation, you'd save the scaler used for outcome prediction\n",
    "    pickle.dump(scaler_t, f)  # Using treatment scaler as placeholder\n",
    "\n",
    "with open('../models/treatment_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler_t, f)\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'outcome_model': {\n",
    "        'name': outcome_trainer.best_model_name,\n",
    "        'cv_score': outcome_trainer.best_score,\n",
    "        'test_score': outcome_trainer.best_model.score(X_test, y_test),\n",
    "        'features': selected_features\n",
    "    },\n",
    "    'treatment_model': {\n",
    "        'name': treatment_trainer.best_model_name,\n",
    "        'cv_score': treatment_trainer.best_score,\n",
    "        'test_score': treatment_trainer.best_model.score(X_test_t_scaled, y_test_t),\n",
    "        'features': X_treatment.columns.tolist()\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"All models and metadata saved successfully!\")\n",
    "print(f\"\\nModel files saved:\")\n",
    "print(f\"- ../models/outcome_prediction_model.pkl\")\n",
    "print(f\"- ../models/treatment_recommendation_model.pkl\")\n",
    "print(f\"- ../models/outcome_scaler.pkl\")\n",
    "print(f\"- ../models/treatment_scaler.pkl\")\n",
    "print(f\"- ../models/model_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. OUTCOME PREDICTION MODEL:\")\n",
    "print(f\"   - Best Model: {outcome_trainer.best_model_name}\")\n",
    "print(f\"   - Cross-Validation Score: {outcome_trainer.best_score:.4f}\")\n",
    "print(f\"   - Test Set Score: {outcome_trainer.best_model.score(X_test, y_test):.4f}\")\n",
    "print(f\"   - Features Used: {len(selected_features)}\")\n",
    "\n",
    "print(f\"\\n2. TREATMENT RECOMMENDATION MODEL:\")\n",
    "print(f\"   - Best Model: {treatment_trainer.best_model_name}\")\n",
    "print(f\"   - Cross-Validation Score: {treatment_trainer.best_score:.4f}\")\n",
    "print(f\"   - Test Set Score: {treatment_trainer.best_model.score(X_test_t_scaled, y_test_t):.4f}\")\n",
    "print(f\"   - Features Used: {X_treatment.shape[1]}\")\n",
    "\n",
    "print(f\"\\n3. MODEL COMPARISON:\")\n",
    "print(f\"   - Models Trained: {len(outcome_models)}\")\n",
    "print(f\"   - Algorithms: {', '.join(outcome_models.keys())}\")\n",
    "print(f\"   - Hyperparameter Tuning: {'Yes' if tuned_model else 'No'}\")\n",
    "\n",
    "print(f\"\\n4. EVALUATION METRICS:\")\n",
    "print(f\"   - Primary Metric: Accuracy\")\n",
    "print(f\"   - Cross-Validation: 5-fold\")\n",
    "print(f\"   - Additional Metrics: Precision, Recall, F1-score\")\n",
    "\n",
    "print(f\"\\n5. NEXT STEPS:\")\n",
    "print(f\"   - Models ready for deployment\")\n",
    "print(f\"   - Use recommend.py for making predictions\")\n",
    "print(f\"   - Proceed to evaluation notebook for detailed analysis\")\n",
    "\n",
    "print(f\"\\nTraining completed successfully! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}