{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Results Analysis\n",
    "\n",
    "This notebook covers:\n",
    "- Comprehensive model evaluation\n",
    "- Performance visualization\n",
    "- Error analysis\n",
    "- Model interpretation\n",
    "- Final recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from evaluate_model import ModelEvaluator, OutcomeEvaluator, TreatmentEvaluator\n",
    "from recommend import TreatmentRecommendationSystem\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "with open('../models/outcome_prediction_model.pkl', 'rb') as f:\n",
    "    outcome_model = pickle.load(f)\n",
    "\n",
    "with open('../models/treatment_recommendation_model.pkl', 'rb') as f:\n",
    "    treatment_model = pickle.load(f)\n",
    "\n",
    "# Load model metadata\n",
    "with open('../models/model_metadata.json', 'r') as f:\n",
    "    model_metadata = json.load(f)\n",
    "\n",
    "print(\"Models loaded successfully!\")\n",
    "print(f\"\\nOutcome Model: {model_metadata['outcome_model']['name']}\")\n",
    "print(f\"Treatment Model: {model_metadata['treatment_model']['name']}\")\n",
    "\n",
    "# Load test data\n",
    "X_test = pd.read_csv('../data/processed/X_test_final.csv')\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv')['y_test']\n",
    "\n",
    "print(f\"\\nTest data loaded: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize evaluators\n",
    "outcome_evaluator = OutcomeEvaluator()\n",
    "\n",
    "# Define labels\n",
    "outcome_labels = ['Improved', 'Not Improved', 'Stable']\n",
    "feature_names = X_test.columns.tolist()\n",
    "\n",
    "# Comprehensive evaluation of outcome model\n",
    "outcome_results, outcome_cm, outcome_feature_imp = outcome_evaluator.generate_evaluation_report(\n",
    "    outcome_model, X_test, y_test, feature_names, outcome_labels, \n",
    "    model_metadata['outcome_model']['name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed error analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_pred = outcome_model.predict(X_test)\n",
    "y_pred_proba = outcome_model.predict_proba(X_test) if hasattr(outcome_model, 'predict_proba') else None\n",
    "\n",
    "# Create error analysis dataframe\n",
    "error_df = X_test.copy()\n",
    "error_df['true_outcome'] = y_test\n",
    "error_df['predicted_outcome'] = y_pred\n",
    "error_df['correct_prediction'] = (y_test == y_pred)\n",
    "\n",
    "if y_pred_proba is not None:\n",
    "    error_df['prediction_confidence'] = np.max(y_pred_proba, axis=1)\n",
    "\n",
    "# Analyze misclassifications\n",
    "misclassified = error_df[error_df['correct_prediction'] == False]\n",
    "print(f\"Total misclassifications: {len(misclassified)} out of {len(error_df)} ({len(misclassified)/len(error_df)*100:.1f}%)\")\n",
    "\n",
    "# Misclassification patterns\n",
    "print(\"\\nMisclassification patterns:\")\n",
    "misclass_patterns = pd.crosstab(misclassified['true_outcome'], misclassified['predicted_outcome'], \n",
    "                               margins=True, margins_name='Total')\n",
    "print(misclass_patterns)\n",
    "\n",
    "# Visualize error patterns\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(2, 3, 1)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=outcome_labels, yticklabels=outcome_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# Prediction confidence distribution\n",
    "if y_pred_proba is not None:\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.hist(error_df[error_df['correct_prediction']]['prediction_confidence'], \n",
    "             alpha=0.7, label='Correct', bins=20)\n",
    "    plt.hist(error_df[~error_df['correct_prediction']]['prediction_confidence'], \n",
    "             alpha=0.7, label='Incorrect', bins=20)\n",
    "    plt.xlabel('Prediction Confidence')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Confidence Distribution')\n",
    "    plt.legend()\n",
    "\n",
    "# Error rate by confidence threshold\n",
    "if y_pred_proba is not None:\n",
    "    plt.subplot(2, 3, 3)\n",
    "    confidence_thresholds = np.arange(0.3, 1.0, 0.05)\n",
    "    error_rates = []\n",
    "    sample_sizes = []\n",
    "    \n",
    "    for threshold in confidence_thresholds:\n",
    "        high_conf_mask = error_df['prediction_confidence'] >= threshold\n",
    "        if high_conf_mask.sum() > 0:\n",
    "            error_rate = 1 - error_df[high_conf_mask]['correct_prediction'].mean()\n",
    "            error_rates.append(error_rate)\n",
    "            sample_sizes.append(high_conf_mask.sum())\n",
    "        else:\n",
    "            error_rates.append(np.nan)\n",
    "            sample_sizes.append(0)\n",
    "    \n",
    "    plt.plot(confidence_thresholds, error_rates, 'o-')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.title('Error Rate vs Confidence Threshold')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "# Feature importance for misclassified samples\n",
    "if outcome_feature_imp is not None:\n",
    "    plt.subplot(2, 3, 4)\n",
    "    top_features = outcome_feature_imp.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Most Important Features')\n",
    "\n",
    "# Class-wise performance\n",
    "plt.subplot(2, 3, 5)\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, y_pred, target_names=outcome_labels, output_dict=True)\n",
    "metrics_df = pd.DataFrame(report).T.iloc[:-3]  # Exclude avg rows\n",
    "metrics_df[['precision', 'recall', 'f1-score']].plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Class-wise Performance Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Sample size by confidence\n",
    "if y_pred_proba is not None:\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(confidence_thresholds, sample_sizes, 'o-', color='orange')\n",
    "    plt.xlabel('Confidence Threshold')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Sample Size vs Confidence Threshold')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Feature importance analysis\n",
    "if outcome_feature_imp is not None:\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(\"-\" * 40)\n",
    "    for idx, row in outcome_feature_imp.head(10).iterrows():\n",
    "        print(f\"{row['feature']:<25}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Feature importance visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    top_15_features = outcome_feature_imp.head(15)\n",
    "    plt.barh(range(len(top_15_features)), top_15_features['importance'])\n",
    "    plt.yticks(range(len(top_15_features)), top_15_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    cumulative_importance = np.cumsum(outcome_feature_imp['importance'])\n",
    "    plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance)\n",
    "    plt.axhline(y=0.8, color='r', linestyle='--', label='80% threshold')\n",
    "    plt.axhline(y=0.9, color='orange', linestyle='--', label='90% threshold')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Cumulative Importance')\n",
    "    plt.title('Cumulative Feature Importance')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find number of features for 80% and 90% importance\n",
    "    features_80 = np.argmax(cumulative_importance >= 0.8) + 1\n",
    "    features_90 = np.argmax(cumulative_importance >= 0.9) + 1\n",
    "    \n",
    "    print(f\"\\nFeatures needed for 80% importance: {features_80}\")\n",
    "    print(f\"Features needed for 90% importance: {features_90}\")\n",
    "    print(f\"Total features: {len(outcome_feature_imp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recommendation System Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATION SYSTEM TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample patients for testing\n",
    "sample_patients = [\n",
    "    {\n",
    "        'patient_id': 1001,\n",
    "        'age': 35,\n",
    "        'gender': 'Female',\n",
    "        'symptoms': 'Fatigue',\n",
    "        'diagnosis': 'Diabetes',\n",
    "        'previous_treatment': 'None',\n",
    "        'severity': 'Mild'\n",
    "    },\n",
    "    {\n",
    "        'patient_id': 1002,\n",
    "        'age': 65,\n",
    "        'gender': 'Male',\n",
    "        'symptoms': 'Cough',\n",
    "        'diagnosis': 'Hypertension',\n",
    "        'previous_treatment': 'Medication A',\n",
    "        'severity': 'Severe'\n",
    "    },\n",
    "    {\n",
    "        'patient_id': 1003,\n",
    "        'age': 45,\n",
    "        'gender': 'Female',\n",
    "        'symptoms': 'Headache',\n",
    "        'diagnosis': 'Depression',\n",
    "        'previous_treatment': 'Therapy',\n",
    "        'severity': 'Moderate'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Testing recommendation system with sample patients...\")\n",
    "print(\"\\nNote: This is a demonstration of the recommendation system structure.\")\n",
    "print(\"In a production environment, you would:\")\n",
    "print(\"1. Load the trained models using the TreatmentRecommendationSystem class\")\n",
    "print(\"2. Preprocess new patient data\")\n",
    "print(\"3. Generate recommendations and outcome predictions\")\n",
    "\n",
    "# Display sample patient profiles\n",
    "for i, patient in enumerate(sample_patients, 1):\n",
    "    print(f\"\\nSample Patient {i}:\")\n",
    "    print(\"-\" * 20)\n",
    "    for key, value in patient.items():\n",
    "        if key != 'patient_id':\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Demonstrate prediction on test data\n",
    "print(f\"\\n\\nDemonstration with actual test data:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Get predictions for first 5 test samples\n",
    "sample_predictions = outcome_model.predict(X_test.head())\n",
    "sample_probabilities = outcome_model.predict_proba(X_test.head()) if hasattr(outcome_model, 'predict_proba') else None\n",
    "\n",
    "for i in range(5):\n",
    "    true_outcome = y_test.iloc[i]\n",
    "    pred_outcome = sample_predictions[i]\n",
    "    \n",
    "    print(f\"\\nTest Sample {i+1}:\")\n",
    "    print(f\"  True Outcome: {outcome_labels[true_outcome]}\")\n",
    "    print(f\"  Predicted Outcome: {outcome_labels[pred_outcome]}\")\n",
    "    print(f\"  Correct: {'âœ“' if true_outcome == pred_outcome else 'âœ—'}\")\n",
    "    \n",
    "    if sample_probabilities is not None:\n",
    "        confidence = sample_probabilities[i][pred_outcome]\n",
    "        print(f\"  Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE BENCHMARKING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare with baseline models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Create baseline models\n",
    "baseline_models = {\n",
    "    'Random': DummyClassifier(strategy='uniform', random_state=42),\n",
    "    'Most Frequent': DummyClassifier(strategy='most_frequent'),\n",
    "    'Stratified': DummyClassifier(strategy='stratified', random_state=42)\n",
    "}\n",
    "\n",
    "# Train baseline models\n",
    "baseline_results = {}\n",
    "for name, model in baseline_models.items():\n",
    "    model.fit(X_test, y_test)  # Using test set for baseline (just for comparison)\n",
    "    y_pred_baseline = model.predict(X_test)\n",
    "    \n",
    "    baseline_results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_baseline),\n",
    "        'precision': precision_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        'f1': f1_score(y_test, y_pred_baseline, average='weighted')\n",
    "    }\n",
    "\n",
    "# Our model performance\n",
    "our_model_results = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "    'recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "    'f1': f1_score(y_test, y_pred, average='weighted')\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for model_name, metrics in baseline_results.items():\n",
    "    for metric_name, value in metrics.items():\n",
    "        comparison_data.append({\n",
    "            'Model': f'Baseline - {model_name}',\n",
    "            'Metric': metric_name.title(),\n",
    "            'Score': value\n",
    "        })\n",
    "\n",
    "for metric_name, value in our_model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': f'Our Model - {model_metadata[\"outcome_model\"][\"name\"]}',\n",
    "        'Metric': metric_name.title(),\n",
    "        'Score': value\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pivot_df = comparison_df.pivot(index='Model', columns='Metric', values='Score')\n",
    "sns.heatmap(pivot_df, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(data=comparison_df, x='Metric', y='Score', hue='Model')\n",
    "plt.title('Performance Metrics Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(pivot_df.round(4))\n",
    "\n",
    "# Calculate improvement over best baseline\n",
    "best_baseline_acc = max([results['accuracy'] for results in baseline_results.values()])\n",
    "improvement = our_model_results['accuracy'] - best_baseline_acc\n",
    "print(f\"\\nImprovement over best baseline: {improvement:.4f} ({improvement/best_baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\n1. MODEL PERFORMANCE:\")\n",
    "print(f\"   - Model: {model_metadata['outcome_model']['name']}\")\n",
    "print(f\"   - Test Accuracy: {our_model_results['accuracy']:.4f}\")\n",
    "print(f\"   - Test Precision: {our_model_results['precision']:.4f}\")\n",
    "print(f\"   - Test Recall: {our_model_results['recall']:.4f}\")\n",
    "print(f\"   - Test F1-Score: {our_model_results['f1']:.4f}\")\n",
    "\n",
    "# Cross-validation performance\n",
    "print(f\"\\n2. CROSS-VALIDATION PERFORMANCE:\")\n",
    "print(f\"   - CV Accuracy: {model_metadata['outcome_model']['cv_score']:.4f}\")\n",
    "print(f\"   - CV-Test Gap: {abs(model_metadata['outcome_model']['cv_score'] - our_model_results['accuracy']):.4f}\")\n",
    "print(f\"   - Overfitting: {'Low' if abs(model_metadata['outcome_model']['cv_score'] - our_model_results['accuracy']) < 0.05 else 'Moderate' if abs(model_metadata['outcome_model']['cv_score'] - our_model_results['accuracy']) < 0.1 else 'High'}\")\n",
    "\n",
    "# Feature analysis\n",
    "if outcome_feature_imp is not None:\n",
    "    print(f\"\\n3. FEATURE ANALYSIS:\")\n",
    "    print(f\"   - Total features: {len(outcome_feature_imp)}\")\n",
    "    print(f\"   - Features for 80% importance: {features_80}\")\n",
    "    print(f\"   - Most important feature: {outcome_feature_imp.iloc[0]['feature']}\")\n",
    "    print(f\"   - Top feature importance: {outcome_feature_imp.iloc[0]['importance']:.4f}\")\n",
    "\n",
    "# Error analysis summary\n",
    "print(f\"\\n4. ERROR ANALYSIS:\")\n",
    "print(f\"   - Total misclassifications: {len(misclassified)} ({len(misclassified)/len(error_df)*100:.1f}%)\")\n",
    "print(f\"   - Most confused classes: {misclass_patterns.iloc[:-1, :-1].values.max()} misclassifications\")\n",
    "\n",
    "if y_pred_proba is not None:\n",
    "    avg_confidence = error_df['prediction_confidence'].mean()\n",
    "    correct_confidence = error_df[error_df['correct_prediction']]['prediction_confidence'].mean()\n",
    "    incorrect_confidence = error_df[~error_df['correct_prediction']]['prediction_confidence'].mean()\n",
    "    \n",
    "    print(f\"   - Average prediction confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"   - Correct predictions confidence: {correct_confidence:.3f}\")\n",
    "    print(f\"   - Incorrect predictions confidence: {incorrect_confidence:.3f}\")\n",
    "\n",
    "# Baseline comparison\n",
    "print(f\"\\n5. BASELINE COMPARISON:\")\n",
    "print(f\"   - Best baseline accuracy: {best_baseline_acc:.4f}\")\n",
    "print(f\"   - Our model accuracy: {our_model_results['accuracy']:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:.4f} ({improvement/best_baseline_acc*100:.1f}%)\")\n",
    "\n",
    "# Recommendations\n",
    "print(f\"\\n6. RECOMMENDATIONS:\")\n",
    "if our_model_results['accuracy'] > 0.8:\n",
    "    print(f\"   âœ“ Model shows good performance (>80% accuracy)\")\n",
    "elif our_model_results['accuracy'] > 0.7:\n",
    "    print(f\"   âš  Model shows moderate performance (70-80% accuracy)\")\n",
    "else:\n",
    "    print(f\"   âœ— Model needs improvement (<70% accuracy)\")\n",
    "\n",
    "if abs(model_metadata['outcome_model']['cv_score'] - our_model_results['accuracy']) < 0.05:\n",
    "    print(f\"   âœ“ Low overfitting - model generalizes well\")\n",
    "else:\n",
    "    print(f\"   âš  Consider regularization to reduce overfitting\")\n",
    "\n",
    "if improvement > 0.1:\n",
    "    print(f\"   âœ“ Significant improvement over baseline models\")\n",
    "else:\n",
    "    print(f\"   âš  Consider more advanced techniques or feature engineering\")\n",
    "\n",
    "print(f\"\\n7. DEPLOYMENT READINESS:\")\n",
    "deployment_score = 0\n",
    "if our_model_results['accuracy'] > 0.75:\n",
    "    deployment_score += 1\n",
    "if abs(model_metadata['outcome_model']['cv_score'] - our_model_results['accuracy']) < 0.05:\n",
    "    deployment_score += 1\n",
    "if improvement > 0.05:\n",
    "    deployment_score += 1\n",
    "\n",
    "if deployment_score >= 2:\n",
    "    print(f\"   âœ“ Model is ready for deployment\")\n",
    "    print(f\"   âœ“ Consider A/B testing in production\")\n",
    "else:\n",
    "    print(f\"   âš  Model needs further improvement before deployment\")\n",
    "    print(f\"   âš  Consider collecting more data or trying different algorithms\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"EVALUATION COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
    "print(f\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}